---
title: My first win with agents
date: 2025-12-13T11:59:38-03:00
layout: post
lang: en
tags: [ai, projects]
draft: true
excerpt: "I built a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?"
image: giralibros.png
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

Previously on /Facundo's Adventures with LLMs/:

- [[https://jorge.olano.dev/blog/on-ai-assistance/][On AI assistance]] (2024-03): I start to occasionally reach to ChatGPT while working on a new open-source project, especially for chores like "I wish there already was a lib for this", "I need a Makefile recipe for this", etc. I see the potential; the results are hit-and-miss. I make plans to better integrate this tech into my workflow.
- [[augmentation-replacement][Augmentation / Replacement]] (2025-03): I added [[https://github.com/karthink/gptel][gptel]] to my Emacs setup and see myself using LLMs daily, as a rubber duck and Google's L1 Cache. I recognize the human augmentation potential of AI---not so much for what LLMs are today, but for the glimpse we get of what a better AI could be. On the other hand, the current push to use LLMs as human replacements seems short-sighted and counter-productive.
- [[agentic-coding-experience][Quick notes on a brief agentic coding experience]] (2025-06): I naively try to use Claude Code to implement some features of an already working web application, burn some cash in the process, and get nothing much out of it.

This time I will document a successful experience I had building a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?

** Part I: the project
The project is a book trading webapp for the Buenos Aires local area.

#+BEGIN_EXPORT html
<div class="text-center">
<img src="{{site.config.static_root}}/img/giralibros.png"></a>
</div>
#+END_EXPORT


1. Users publish a list of books they offer for trading, and browse other user's books.
2. When a user sees something they like, they send an exchange request to the book owner, who receives it as an email notification.
3. If the owner is interested in any of the other user's books, they arrange to meet and make the trade. This exchange takes place outside of the application (there are no incentives to keep users in-app; email and WhatsApp are more convenient for messaging).

I wrote this application from scratch[fn:1] using Django, SQLite for the database, and Bulma for styles. It runs on a small Debian server behind nginx.

*** Goals
- *Finishing the project*, with the specific UX I had in mind (which was very simple).
- *Minimizing the effort* I had to make to implement it (counting the frustration and disgust with the tooling, e.g. Claude Code, as part of that effort).
- *Minimizing operational costs* to run the system: if this was successful I would run it as a free community project, so I needed to design it to be cheap to run and not take much of my time.
- *Keeping a decent understanding of the codebase* (at least of its backend).

*** Non-goals
There are a number of things I typically go for in personal projects, which I explicitly didn't care about for this one:
- Maximizing speed: as long as I got it finished with low effort, I was in no rush.
- Having fun.
- Learning.
- Ensuring long-term maintainability, flexibility, or extensibility of the codebase: in a way this was a proof of concept. I wanted to get it out there and see if people liked it. It's small enough that I can risk not having a strong hold of the codebase, because it wouldn't be hard to quickly rewrite it if necessary.
- Building a successful product: I wanted this to succeed, not least because I wanted to use it to trade books but, other than making the application free and accessible, it was out of my hands whether people adopt it or not---I wouldn't go out of my way to promote it, either.
- Keeping users happy: since this wasn't a business, I could afford to miss features, ship bugs, lose data, etc. I got nothing to lose.

While all the things on this second list are desirable, I was willing to trade them off for those on the first one.

*** Using agents
Given the specific mix of goals and non-goals, this seemed like a good opportunity to have another try at
building with agents, instead of writing all the code myself[fn:3]. I could afford some of the risks I associate with delegating too much to the AI---shipping something I don't fully understand, that could take me an extra effort to fix when it breaks, or that turns out to be unmaintainable in the long run.

A few things had changed since my previous previous experiment with Claude Code:
- I learned about the 20USD/month Pro plan, which was more reasonable for personal projects than the Max plans or the API key alternative.
- I kept reading accounts from other (often skeptical) developers, which gave me new ideas and better context to work with these tools.
- This time I would work on a greenfield project, which is where agents shine. The stack was more LLM-friendly, too: the Django guardrails plus vanilla JavaScript are a better match to their training set than my previous Flask/SQLAlchemy/HTMX/hyperscript extravaganza.

Django was an ideal fit for me to leverage LLMs: I used Django intensively for 5 years... a decade ago. I still have a good grasp of its concepts, a notion of what's doable with it, what's built-in and what should be ad hoc, etc., but I completely forgot the ORM syntax, the class names, the little details. I could instruct Claude exactly what to do, saving me a lot of documentation roundtrips while still catching whenever it tried to bullshit me into getting creative or reinventing the wheel.

For the front-end the risk/reward was a bit higher. I've been officially a backend dev for a while now,
and while I've used Bulma on a few projects and have a good idea of what it offers, I'm not trained to review HTML and CSS, so it was likelier that Claude would slip working, superficially good-looking front end code that would quickly degrade and become an unmaintainable mess. On the other hand, and for the same reasons, despite my best intentions the HTML and CSS I produce manually tends to be less maintainable anyway---Claude would just accelerate the cycle. This turned out to be a very good trade-off, since Claude allowed me to iterate quickly in prototype mode and arrive to a look-and-feel that fit the project, something that would have taken me a big effort if I was writing the HTML myself.

*** Results
I released an MVP after one week of part-time work, and a few extra nice to haves in another week. At the time of writing, there over 80 registered users and 400 offered books. The app doesn't track this, but I know first hand of a few book exchanges that already took place.

In terms of operational costs, which I tried to keep low:
  - 6 USD/month on a Hetzner VPS
  - 7 USD/year on a Porkbun domain
  - 2.5 USD/every 6 months for ZeptoMail credits

This 7 USD/month total is less than what a second hand book costs here.

** Part II: the process
I really liked the recent [[https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/][/A Month of Chat-Oriented Programming/]] post and I borrowed a few ideas from it. I like the notion of chat-oriented programming as opposed to vibe-coding. That's what I tried to do with this project, although my own variation of it, which I describe below.

*** 1. Skip agent setup
I made a very deliberate choice not to invest in agent customization and support tooling, or whatever that's called: no fancy CLAUDE.md instructions, no MCP servers, no custom commands, no system prompts, no skills, no plugins. I'm not saying these aren't useful, but I find them to be non-conducive rabbit holes: in my previous experiments I ended up spending a lot of tokens trying to come up with an ideal workflow specification, which claude would still randomly forget or ignore.

A fundamental problem is that the agent doesn't seem to have much knowledge about itself, its configuration, or its commands. When deep in conversational mode, one feels inclined to approach tweaking the tool in the same way, asking the tool to explain how to best use it or telling it to adjust a specific configuration, only to find it has no clue about how to do that. In that context, tweaking the agent at will requires a big context switch; in my opinion, spending mental cycles in such meta-tasks defeats the purpose of coding with agents. Since these tools are changing every month, I'm not compelled to make an investment unless it yields immediate results. My attitude then was: see how far this can get me today with minimal tweaking; if that's not very far, I'll just wait and try again in a few months.

*** 2. Switch tactically between default, edit, and plan mode
For non trivial features I started the session with some product-level context of the requirement, and the subset of files relevant to the implementation. Then I bounced some ideas with Claude, sometimes providing a succinct TODO list of things that I expected to change in the app or the code. My goal was to minimize the opportunities for it to get creative or improvise, while still giving it room to catch weak spots in my reasoning.

For the few more involved features I worked exclusively in plan mode, having Claude Code produce a markdown file as the output of the session, to be picked up on a separate session.

*** 3. Claude writes the code, I commit
I reviewed all code before each commit, asking for refactors of the obvious mistakes, and again before merging the PR. This process pushed me to break the tasks down into obvious increments that would make good checkpoints for committing, which I made part of my instructions (this is about the same I do when I'm writing the code). When Claude was working, I kept an eye on the console output and interrupted it when it looked like it was trying to do too much at once.

*** 4. I decide what and how to test
I find that an explicit test exercising every relevant "business rule" is more effective than documentation, code comments, and the overall design/architecture in capturing the desired system behavior and guaranteeing that it  does what it is supposed to do. This is even more important in the context of agentic coding, where I'm voluntarily resigning some control over the implementation.

I mostly agree with the sacred rule in [[https://diwank.space/field-notes-from-shipping-real-code-with-claude][/Field Notes From Shipping Real Code With Claude/]]: /Never. Let. AI. Write. Your. Tests./ I was slightly less strict, though: instead of writing the tests myself, I provided a set of rules and step-by-step outlines of the integration tests I wanted:

#+begin_src python
def request_book_exchange(self):
    # register two users
    # first user with 3 books
    # second user two books
    # send request for second book
    # check outgoing email
    # check email content includes 2nd user contact details
    # check email content lists user books
    pass

def mark_as_already_requested(self):
    # register two users
    # first user with 3 books
    # second user gets home, sees all three books and Change button
    # send request for second book
    # request list shows 2 Change, one already request
    pass
#+end_src

Then I carefully reviewed the implementation to ensure it followed my testing preferences: don't couple to implementation details (test units of behavior, not units of code), don't mock intermediate layers (just the inputs and outputs of your system, i.e. its observable behavior), don't access the DB directly[fn:2]. Once a few tests were in place, Claude was less likely to deviate from the surrounding style.

I also did some smoke testing after each feature was ready to merge. I haven't experimented with something like Playwright, but I suspect that would be a good addition to prevent regressions in the UI, which is where most of the application complexity resides.

*** 5. Don't let Claude debug on its own

For an unsophisticated project like this, with good enough detail in the prompts, Claude tends to get the implementation right or almost right let's say 80% of the time.

I noticed that, when something fails and the problem isn't obvious, Claude can figure out the problem on its own maybe 30% of the times. This can include very subtle or cryptic errors that could take me hours to resolve myself. The problem is that remaining 70%. I find that the LLM, even with the command line at its disposal, if left unchecked will  be both clueless and eager to try things at random, accumulating layers of useless changes, going in circles and very far from the actual problem.

What worked for me is to give it one shot to figure out the issue autonomously and, when that fails, take over, not necessarily to do the full debugging and fix myself but to feed it plausible hypotheses and evidence, to put it back on track to a solution.

*** 6. Don't repeat yourself (but sometimes do)

Code duplication is an interesting thing to reflect about in the context of working with agents. LLMs get paid (?) to output tokens, so unsurprisingly Claude Code indulges in all kinds of duplication, from repeating patterns found in the same module to reimplementing entire chunks of the Django built-ins its already importing. It would be tempting to add strict rules to CLAUDE.md to prevent all code duplication, but as we collectively learned in the last decade, dogmatically applying DRY tends to do more harm than good.

The anniversary edition of /The Pragmatic Programmer/ makes a useful distinction between duplication /of code/ and duplication /of knowledge/, the latter being what we need to be more wary of. In the context of coding with LLMs, where (re)writing code is effortless but scattered knowledge is a threat to system survival,
the distinction is fundamental. So I found most of my reviews deal with strategically removing or allowing duplication: if it's knowledge, it needs to be centralized and I need to think carefully how; if it's just code, I can instruct how to extract and reuse, but more often than not it's better just live with that duplication.

*** 7. Plan around token and session limits

There are a few usage limits that need to be taken into account when working with Claude Code.

The first is the size of the conversation context window. There is so much context the model can fit when processing a new message, and a long running session will eventually exhaust it. By default, CC will try to "compact" the context to keep it manageable but, as noted in the /Month of CHOP/ post, this degrades the quality of its output. I also found that the process of compaction itself spends a lot of tokens, which is problematic because of the Plan usage limits.

I followed /Month of CHOP/ post advice to turn off autocompaction in the settings, and kept an eye on its token consumption via the ~/context~ command, which looks like this:
#+begin_src
> /context
  ⎿
  ⎿   Context Usage
  ⎿  ⛁ ⛀ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛀   claude-sonnet-4-5-20250929 · 80k/200k tokens (40%)
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁   ⛁ System prompt: 3.1k tokens (1.6%)
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁   ⛁ System tools: 15.3k tokens (7.7%)
  ⎿  ⛁ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛁ Memory files: 1.8k tokens (0.9%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛁ Messages: 60.0k tokens (30.0%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛶ Free space: 120k (59.8%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿
  ⎿  Memory files · /memory
  ⎿  └ Project (/Users/facundo/dev/facundoolano/giralibros/CLAUDE.md): 1.8k tokens
  ⎿
  ⎿  SlashCommand Tool · 1 commands
  ⎿  └ Total: 981 tokens
  ⎿
  ⎿
#+end_src

In addition to the model context window, Claude Pro has limits on how much you can consume in session and weekly time windows, which can be monitored at [[claude.ai/settings/usage][https://claude.ai/settings/usage]].

#+BEGIN_EXPORT html
<div class="text-center">
<img src="{{site.config.static_root}}/img/claudeplan.png"></a>
</div>
#+END_EXPORT


I've observed that I exhaust the session limit after a couple of hours of steady work (having to wait 3-5 hours to resume), and a weekly plan limit if I work ~4-5 days in a row, so I learned to plan my work around these constraints.

- Much like I split coding tasks into commit-sized steps, I split features into PR-sized sessions.
- I tried to work on a single thing at a time, e.g. resisting the temptation to add extra/unrelated tasks as they popped to mind. Which was a good idea, anyway, both for project organization and to keep the agent focused on the task at hand.
- I monitored the context window, restarting Claude to "checkpoint" when I was done with a feature and wanted to start on another.
- For bigger tasks that I anticipated wouldn't fit in the context window or session limit, I would do one or more planning sessions first, with a detailed markdown plan as output that could be picked up on a subsequent implementation session.

While it was very annoying to hit these limits at first, I think it eventually pushed me to stay methodical.
If I ran out of tokens and I wanted to continue working in the project, I would switch to non-AI driven activities (plan, research, stub tests, server config, etc.). I find this was a healthy balance for me.

If this was my job and not a side project, and I wanted to improve my throughput, rather than switching to a 100 USD Max plan, I would combine this Pro plan with Codex Plus from OpenAI (also 20USD/month, and would give me exposure to a different model).

*** Conclusions

The process just described may sound like heavy work and a lot of hand-holding, and it's probably not what the "pros" are doing out there with agents but, as stated before, the goal here was not to maximize velocity or throughput but to get a finished product with minimal effort and frustration. I have a fair amount of experience in high-level task breakdown, writing tickets for others to work on, doing superficial code reviews, anticipating pitfalls, and building confidence on a shared codebase through a solid test suite. So this played to my strengths and mostly prevented the LLM from digging itself into a hole that I'd have to get it out of. This micromanaging approach turned out very effective and low effort, at times even rewarding---to see features that sounded complicated at first, and that I would have postponed, work out in a few strokes, was stimulating. It highlighted how much more skill is at play in building a piece of software, beyond code writing. I occasionally let myself fall into the illusion that I wielded this powerful tool, one that extended my reach and abstracted unimportant details away.

A few months ago I qualified the feeling I got from building with agents as "exhilarating recklessness" and compared it with going to the casino. This time it felt as if, after accumulating experience for 15 years, I was "spending" some of those savings to get something I wanted. The analogy goes farther: I acknowledged that if all I ever did was working like this, some of my skills would atrophy---I would run out of savings.

I'm sure I made some mistakes by letting Claude do the coding for me, but this was clearly a successful project given my initial goals and the results I got. I still think it wouldn't be wise to use agents much at work, beyond proof-of-concept software---trading short-term productivity for long-term ownership is rarely a good bargain. As for low-stakes projects, I like that the barrier has lowered to ship good-enough software. It's great to cheaply try out different ideas, without prospects of turning them into reliable systems or marketable products.

*** Notes
[fn:2] My rules are listed in [[what-i-think-i-know-about-testing][/What I think I know about testing/]]
and [[unit-testing-principles][/Unit Testing Principles/]]. I asked Claude to read those posts and dump a summary to CLAUDE.md, but I doubt that made much of a difference.

[fn:3] Using AI was not in itself a project goal, though. I first made my mind about spending time on this project; once that was settled, I tried Claude Code as an experiment. Had I had a first few unproductive sessions I would have started over writing all the code myself (perhaps running a higher risk of abandoning the project before down the line).

[fn:1] This is not an original idea: there's a similar application that worked well for a few years, until its owners introduced a paid subscription model. This change stagnated the community---both for free and paid users---, so the motivation for my project was to offer a free replacement to that community.
