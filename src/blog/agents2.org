---
title: Another round of building with agents
date: 2025-12-13T11:59:38-03:00
layout: post
lang: en
tags: [ai, projects]
draft: true
excerpt: "I built a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?"
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

Previously on /Facundo's Adventures with LLMs/:

- [[https://jorge.olano.dev/blog/on-ai-assistance/][On AI assistance]]: I start to occasionally reach to ChatGPT while working on a new open-source project, especially for chores like "I wish there already was a lib for this", "I need a Makefile recipe for this". I see the potential; the results are hit-and-miss. I make plans to better integrate this tech to my workflow.
- [[augmentation-replacement][Augmentation / Replacement]]: I incorporated LLMs to my workflow through Emacs's [[https://github.com/karthink/gptel][gptel]], and see myself using it daily, as a rubber duck and Google's L1 Cache. I recognize the human augmentation potential of AI---not so much for what LLMs are today, but for what they show us a better AI could be. Trying to push them as a human replacements is short-sighted and counter-productive.
- [[agentic-coding-experience][Quick notes on a brief agentic coding experience]]: I naively try to use Claude Code to implement some planned features for an already working web application, burn a lot of API tokens in the process, and get nothing much out of it.

This time I will document a successful experience I had building a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?

*** The project
The project is a small book trading web application for the Buenos Aires local area:
1. Users publish a list of books they offer for trading, and browse other user's offered books.

   [Screenshot]

2. When they see a book they like, the sends an exchange request to the owner. The owner receives it as an email notification.

   [Screenshot]

3. If the owner is interested in any of the other user's books, they get in touch to meet and make the trade. This exchange takes place outside of the application, typically via email or WhatsApp. (There are no incentives to keep them inside the app).

I wrote this application from scratch[fn:1] using Django, SQLite for the database, and Bulma for styles. It runs on a small Debian server behind nginx.

*** Goals
- *Finishing the project*, with the exact UX I had in mind (which was very simple anyway).
- *Minimizing the effort* that took me (counting the frustration and disgust with the tooling, e.g. Claude Code, as part of that effort).
- *Minimizing operational costs* to run the system: if this is successful it will be a free community project, so I need to design it to be cheap to run and not take much time to maintain.
- *Keeping a decent understanding* of the codebase (at least the backend part)

*** Non-goals
There are a number of things I typically go for in personal projects, which I explicitly don't care about for this one:
- Maximizing speed: as long as I get it finished with low effort, I'm in no rush.
- Having fun
- Learning
- Ensuring maintainability, flexibility, or extensibility of the codebase: in a way this is a proof of concept. I want to get it out there and see if people like it. It's small enough that I can risk not having a strong hold of the codebase, because it wouldn't be hard to quickly rewrite it if necessary.
- Building a successful product: I want this to succeed not least because I want to use it to trade books but, other than making the system free and accessible, it's out of my hands whether people choose to use it or not---I'm not going to go out of my way to promote it, either.
- Keeping users happy: since this is not a business I can afford to miss features, ship bugs, lose data, etc. I got nothing to lose.

While all the things on this second list are desirable, I was willing to trade them off for those on the first one.

*** Claude Code
Given this specific mix of goals and non-goals, this seemed like a good fit for building the code with AI instead of coding it myself. I could take some of the risk I associate with delegating too much to the AI---shipping something I don't fully understand, that could take me an extra effort to fix when it breaks, or that turns out to be unmaintainable in the long run[fn:2].

A few things were different, compared to my previous agents experiment:
- <I learn through colleagues about the 20usd suscription (as opposed of the 100/200usd one and the crazy spend of the API key alternative)
- <I kept reading experiences from other (often skeptical) devs which gave me ideas to approach the use of the tool differently
- I suppose CC got a lot better in these few months
- another critical difference is that this time I was working on a greenfield project as opposed to trying to update a preexisting one. the stack was more LLM friendly, I believe. The Django guardrails plus vanilla js are a better match to its training set than Flask, SQLAlchemy and my HTMX/hyperscript extravaganza.

<- note that I used django intensively for 5 years... a decade ago. that made it an interesting fit, because I still have a good grasp of django concepts, a notion what's doable with it, what's builtin and what should be ad hoc, etc., but I completely forgot the syntax and the little details. I could instruct claude precisely what to do, saving me from a lot of documentation roundtrips, while still catching when it tried to bullshit me into getting creative ore reinventing the wheel

- similarly I have an idea of what was supported by bulma, and I could delegate the HTML layout and css to claude
- this will be a clear trade off, I'm a backend dev, I knew I wouldn't be as effective in catching the garbage early on when it came to CSS structure and javascript features, but to be honest I'm not very good at building maintainable interfaces on my own anyway, and it's by far the biggest burden on me for this kind of project, so I was more than willing to make sacrifices on this front.

Using AI was not a goal in itself. I first made my mind about spending time on this project; once that was settled, I tried Claude Code as an experiment. Had I had a first few unproductive sessions I would have started over writing all the code myself (with higher risk of abandoning the project before down the line).

*** CHOP not vibe
- link to recent article
https://diwank.space/field-notes-from-shipping-real-code-with-claude


- my own flavor of CHOP went like this:
  - not only claude doesn't get to do PRs or PR reviews: it doesn't even get to do commits.
  - I typically either provide some product-level context upfront and then either provide or iterate with Claude on a succinct TODO list of the tasks to be done before letting it start coding. the idea is to minimize the opportunities for it to get creative.

(as stated before, the goal here is not maximize velocity but to get a finished product with minimal effort and frustration on my side.
I have a lot of training in high level task breakdown, superficial reviewing, foreseeing pitfalls, and building confidence through a test suite, so this approach turns out to be very effective for my purposes.)

*** Agent customization
  - don't invest on its tooling: no sophisticated CLAUDE.md, no MCP, no custom commands, no system prompts, no skills, no plugins. I'm not saying these aren't important, but in my previous experience I found them to be non-conducive rabbit holes: I ended up spending a lot of tokens trying to come up with an ideal workflow specification; this is especially problematic because the agent is not trained with knowledge about itself, so it's very ineffective at configuring itself. In that context, it would requires a lot mental effort on my side to tune the agent to my satisfaction, and in my opinion, spending mental effort in such meta-tasks defeats the purpose of using agents in the first place, at least in the context of such a short lived hobby project. Additionally, with these beasts any setup seems to go stale every few months, so my attitude was: see how far this can get me now with minimal tweaking; if that's not very far I'll just wait and try again in a few months.

*** Work sessions
- the only thing I found to be fundamental is to follow X advice to turn compaction off. If you reach compaction then that session is dead. And also if you near compaction you'll likely kill any remaining tokens you have in your current session. These constraints made me arrange my work to always be approachable in one session:
  - don't try to do too much at once
  - close and reopen claude as a sort of "checkpointing" when I'm done with a feature and want to start another one.
  - do a plan-only session, with a detailed markdown plan as output for work that seems to complex work on start to finish before running out of context token or session usage limits.

- I find that these restriction to work on self contained sessions, and the need to take breaks when reaching both session and weekly limit made me keep my work organized and avoid the agent subtly dragging me into its spirals of nonsensical coding sprees.

- When I ran out of tokens, since there was still that barrier on familiarizing with the code, I opted to make progress on UX ideas, feature specifications, test stubs and server setup---all pieces that I didn't want the AI to touch. I found this context switching useful to make steady progress.

In a more serious or time-constrained scenario, I would still opt for paying two 20 dollar pro plans instead of a 100 or 200 USD max plan. I haven't tried, but I suspect a combination of Claude Code Pro and Codex Plus is the ideal combination.

*** Testing
I mostly agree the sacred rule on this post
https://diwank.space/field-notes-from-shipping-real-code-with-claude

but I slightly adapted to reduce friction in adding new tests.
- I don't let AI come up with their own tests
- I have specific rules about what I look for in a test (which aren't different from non-ai driven projects). basically what I listed here and discussed in more length here: test units of behavior not units of code, don't couple to implementation details, prefer integration tests over smaller scale unit tests, don't mock intermediate layers (just the inputs and outputs of your system), don't access the DB directly. (I made claude read those and include the rules in its CLAUDE.md)

This is one of the few areas where I hold my opinions strongly. Taking this approach helps me trust my codebase even when I don't fully understand it or there are pieces that I know need improvement. I find an explicit test exercising every relevant business rule beats documentation, comments and the overall design/architecture in providing guarantees that the system does what it needs to do and will continue to over time.

This becomes even more important in the context of agentic coding, where I'm voluntarily resigning control and understanding of the implementation.

- the slight adjustment is that I want to test as much as possible, and writing tests line by line is still burdensome. So I typically write outlines like:
  [Example]
then ask Claude to implement one at a time and I closely review to make sure it's following my rules and not overcomplicating the implementation. after a few are implemented then it's less likely to deviate from the sorrounding style.
I don't let Claude add tests before adding my detailed outline first

I also do a manual smoke testing after each feature is ready to merge. <I haven't experimented with something like playwright yet, but I suspect that would be a better approach to catch UI regressions, where a lot of the complexity of the app resides

*** Don't repeat yourself

Code duplication is an interesting thing to reflect about in the context of working with agents. LLMs are paid (?) to output tokens, so unsurprisingly Claude Code indulges in all kinds of duplication, <not only pieces of code already defined elsewhere in the project (and possibly outside of its current context), but pieces defined right above the code its adding, or in a class its sub-classing, in the django built-ins already imported, etc.

It's tempting to add rigid rules to CLAUDE.md to prevent code duplication, but as we collectively learned in the last decade, dogmatically removing all code duplication tends to do more harm than good.

The anniversary edition of /The Pragmatic Programmer/ makes a useful distinction between duplication /of code/ and duplication /of knowledge/. The latter being what we need to strictly remove. <The situation compounds in the context of coding agents, where (re)writing code is effortless but scattered knowledge is the biggest system risk.

So I found most of my reviews deal with strategically removing or allowing duplication: if it's knowledge it needs to be centralized and I need to think carefully how, if it's just code I can instruct how to extract and reuse, but more often than not it's OK just live with that duplication.

[TODO better examples?]

*** Debugging

Debugging is another interesting one.

For an unsophisticated project like this, with good detail in the prompts, Claude tends to get the implementation right or almost right most of the time---let's say 80%.

I noticed that, when something fails and the problem isn't obvious, Claude can figure out the problem on its own maybe 30% of the times. This can include very subtle or cryptic errors that could take me hours to resolve myself.

The problem is the remaining 70%. I find that the LLM, even with the command line at its disposal, tends to be both clueless and eager to try random things, accumulating layers of useless changes if left unchecked, going in circles and very far from the actual problem.

So what worked for me is to give it one shot to figure out the issue autonomously and, when that fails, take over, not necessarily to implement the fix myself but to feed it plausible hypothesis to test and get it on track to a solution.

*** Results
- released working mvp in 1 (part time) week
- added QoL improvements in another week
- got ~70 users and ~300 books after promoting it with friends and people I exchange books with in the past. The app doesn't track this, but I know first hand that a few book exchanges already happened during the first week.
- cost of running it is
  X/month hetzner vps
  X/year porkbun domain
  X/6 months for zepto emails
  ~ the cost a used book in Buenos Aires.

*** Conclusions

- previous attempt I qualified the feeling as "exhilarating recklessness" and compared the experience with going to the casino.
- this time instead I felt like I was leveraging my experience: like I had been saving in experience for 15 years and Claude allowed me to "spend" some of that experience to get something that I wanted. This analogy works in more than one way:
  - it felt rewarding to see how much I know about this stuff that I had not think about in a long while, how many little nuances about putting together a web app and trying different user interfaces I could think about, how many problems I could catch by just reviewing Claude's code. Even if wasn't writing the code, there was a lot I could bring to the table .
  - I did feel like if I only coded this way my experience would give diminishing returns and my skills would atrophy

- I'm sure there are many mistakes I made by letting claude do the work for me, but this was clearly a successful project given my goals and the results
- I still don't think it would be wise to use this a lot in professional environments, other than for proof of concept-type of software. I don't think that trading short term productivity for long term ownership and knowledge building makes sense in most cases.

*** Notes
[fn:1] This is not an original idea: there's already a similar application that worked pretty well for a few years, until they introduced an aggressive subscription model. This change stagnated the community---both for free and paid users---, so the motivation for my project was to provide a free replacement to that community.

[fn:2] It's not that I planned to "vibe" carelessly; it's more like, once I let go a little, it's hard to keep track when the letting go becomes too much.
