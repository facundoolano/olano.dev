---
title: My first success building with agents
date: 2025-12-13T11:59:38-03:00
layout: post
lang: en
tags: [ai, projects]
draft: true
excerpt: "I built a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?"
image: giralibros.png
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

Previously on /Facundo's Adventures with LLMs/:

- [[https://jorge.olano.dev/blog/on-ai-assistance/][On AI assistance]] (2024-03): I start to occasionally reach to ChatGPT while working on a new open-source project, especially for chores like "I wish there already was a lib for this", "I need a Makefile recipe for this", etc. I see the potential; the results are hit-and-miss. I make plans to better integrate this tech into my workflow.
- [[augmentation-replacement][Augmentation / Replacement]] (2025-03): I added [[https://github.com/karthink/gptel][gptel]] to my Emacs setup and see myself using LLMs daily, as a rubber duck and Google's L1 Cache. I recognize the human augmentation potential of AI---not so much for what LLMs are today, but for the glimpse we get of what a better AI could be. On the other hand, the current push to use LLMs as human replacements seems short-sighted and counter-productive.
- [[agentic-coding-experience][Quick notes on a brief agentic coding experience]] (2024-06): I naively try to use Claude Code to implement some features of an already working web application, burn some cash in the process, and get nothing much out of it.

This time I will document a successful experience I had building a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?

** Part I: the project
The project is a small book trading web application for the Buenos Aires local area.

#+BEGIN_EXPORT html
<div class="text-center">
<img src="{{site.config.static_root}}/img/giralibros.png"></a>
</div>
#+END_EXPORT


1. Users publish a list of books they offer for trading, and browse other user's books.
2. When they see something they like, the user sends an exchange request to the book owner, who receives it as an email notification.
3. If the owner is interested in any of the other user's books, they arrange to meet and make the trade. This exchange takes place outside of the application (there are no incentives to keep users in-app; email and WhatsApp are more convenient).

I wrote this application from scratch[fn:1] using Django, SQLite for the database, and Bulma for styles. It runs on a small Debian server behind nginx.

*** Goals
- *Finishing the project*, with the exact UX I had in mind (which was very simple).
- *Minimizing the effort* I had to make to implement it (counting the frustration and disgust with the tooling, e.g. Claude Code, as part of that effort).
- *Minimizing operational costs* to run the system: if this was successful I would run it as a free community project, so I needed to design it to be cheap to run and not take much of my time.
- *Keeping a decent understanding of the codebase* (at least of its backend).

*** Non-goals
There are a number of things I typically go for in personal projects, which I explicitly didn't care about for this one:
- Maximizing speed: as long as I got it finished with low effort, I was in no rush.
- Having fun.
- Learning.
- Ensuring long-term maintainability, flexibility, or extensibility of the codebase: in a way this was a proof of concept. I wanted to get it out there and see if people liked it. It's small enough that I can risk not having a strong hold of the codebase, because it wouldn't be hard to quickly rewrite it if necessary.
- Building a successful product: I want this to succeed, not least because I want to use it to trade books but, other than making the application free and accessible, it's out of my hands whether people choose to use it or not---I'm not going to go out of my way to promote it, either.
- Keeping users happy: since this is not a business, I can afford to miss features, ship bugs, lose data, etc. I got nothing to lose.

While all the things on this second list are desirable, I was willing to trade them off for those on the first one.

*** Using agents
Given the specific mix of goals and non-goals, this seemed like a opportunity to have another try at
building with agents, instead of writing all the code myself[fn:3]. I could afford taking some of the risks I associate with delegating too much to the AI---shipping something I don't fully understand, that could take me an extra effort to fix when it breaks, or that turns out to be unmaintainable in the long run.

A few things had changed since my previous previous experiment with Claude Code:
- One of the reasons I didn't keep up with Claude Code was that a 100-200 USD/month subscription seemed too much for my occasional side projects, and the API key alternative could easily burn 30 bucks in a day. This time I switched to the more reasonable 20 USD/month Pro plan.
- During this time, I kept reading accounts of agent coding experiences from other (often skeptical) devs, which gave me new ideas and better context to work with these tools.
- Probably the biggest difference, this was going to be a greenfield project, which is where agents shine. The stack was more LLM-friendly too, I believe: the Django guardrails plus vanilla JavaScript are a better match to their training set than Flask, SQLAlchemy and the HTMX/hyperscript extravaganza from my other project.

Django was an ideal fit for me to leverage LLMs: I used Django intensively for 5 years... a decade ago. I still have a good grasp of its concepts, a notion of what's doable with it, what's built-in and what should be ad hoc, etc., but I completely forgot the ORM syntax, the class names, the little details. I could instruct Claude exactly what to do, saving me a lot of documentation roundtrips while still catching whenever it tried to bullshit me into getting creative or reinventing the wheel.

For the front-end the risk/reward was a bit higher. I've been officially a backend dev for a while now,
and while I've used Bulma on a few projects and have a good idea of what it offers, I'm not trained to review HTML and CSS, so it's likelier that Claude would slip working, superficially good-looking front end code that would quickly degrade and become an unmaintainable mess. On the other hand, and for the same reasons, despite my best intentions the HTML and CSS I produce manually tends to be less maintainable anyway---Claude would just accelerate the process. This turned out to be a very good trade off, since Claude allowed me to iterate quickly in prototype mode, to arrive to the look-and-feel that worked well for the project, something that would have taken me a big effort if I was writing the HTML myself.

*** Results
I released an MVP after one week of part-time work, and a few extra nice to haves in another week. At the time of writing, there are ~80 registered users and ~400 offered books. The app doesn't track this, but I know first hand of a few book exchanges that already took place.

The costs of running the app are:
  - 6 USD/month on a Hetzner VPS
  - 7 USD/year on a Porkbun domain
  - 2.5 USD/every 6 months for ZeptoMail credits

The 7 USD/month total is about the cost of a second hand book here.

** Part II: the process
I really liked the recent [[https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/][/A Month of Chat-Oriented Programming/]] post and I borrowed a few ideas from it. I like the notion of chat-oriented programming as opposed to vibe-coding. That's what I tried to do with this project, although my own variation of it.

*** 1. Skip agent setup
I made a very deliberate choice not to invest in agent customization and support tooling, or whatever that's called: no fancy CLAUDE.md instructions, no MCP servers, no custom commands, no system prompts, no skills, no plugins. I'm not saying these aren't useful, but I find them to be non-conducive rabbit holes: in my previous experiments I ended up spending a lot of tokens trying to come up with an ideal workflow specification, that CLAUDE would randomly forget or ignore.

A fundamental problem is that the agent doesn't seem to have much knowledge about itself, its configuration, or its commands. When deep in conversational mode, one feels inclined to approach tweaking the tool in the same way, only to find it has no clue about how to do what you ask it too. In that context, tweaking the agent at will requires a big context switch and, in my opinion, spending mental effort in such meta-tasks defeats the purpose of coding with agents. To compound the problem, the knowledge in this space seems ephemeral, so it's hardly a long-term investment. My attitude then was: see how far this can get me today with minimal tweaking; if that's not very far I'll just wait and try again in a few months.

*** 2. Switch tactically between default, plan, and edit mode
For non trivial features I started the session with some product-level context of the requirement, and the subset of files relevant to the implementation. Then I bounced some ideas with Claude, sometimes providing a succinct TODO list of things that I expected to change in the app or the code. My goal was to minimize the opportunities for it to get creative or improvise, while still giving it room to catch weak spots in my reasoning.

For the few more involved features I worked exclusively in plan mode, having Claude Code produce a markdown file as the output of the session, to be picked up on a separate session.

*** 3. Claude writes all the code, but never commits
I reviewed all code before each commit, asking for refactors of the obvious mistakes, and again before merging the PR. This process pushed me to break the tasks down into obvious increments that would make good checkpoints for committing, which I made part of my instructions. When Claude was working, I kept an eye on the console output and interrupted it when it looked like it was trying to do too much at once.

*** 4. The human decides what and how to test
I tried to test any relevant "business rule" through HTTP-level integration tests. I did let Claude write test code but providing step-by-step outlines.

TODO cleanup below

I mostly agree the sacred rule on this post
https://diwank.space/field-notes-from-shipping-real-code-with-claude

but I slightly adapted to reduce friction in adding new tests.
- I don't let AI come up with their own tests
- I have specific rules about what I look for in a test (which aren't different from non-ai driven projects). basically what I listed here and discussed in more length here: test units of behavior not units of code, don't couple to implementation details, prefer integration tests over smaller scale unit tests, don't mock intermediate layers (just the inputs and outputs of your system), don't access the DB directly. (I made claude read those and include the rules in its CLAUDE.md)

This is one of the few areas where I hold my opinions strongly. Taking this approach helps me trust my codebase even when I don't fully understand it or there are pieces that I know need improvement. I find an explicit test exercising every relevant business rule beats documentation, comments and the overall design/architecture in providing guarantees that the system does what it needs to do and will continue to over time.

This becomes even more important in the context of agentic coding, where I'm voluntarily resigning control and understanding of the implementation.

- the slight adjustment is that I want to test as much as possible, and writing tests line by line is still burdensome. So I typically write outlines like:
  [Example]
then ask Claude to implement one at a time and I closely review to make sure it's following my rules and not overcomplicating the implementation. after a few are implemented then it's less likely to deviate from the sorrounding style.
I don't let Claude add tests before adding my detailed outline first

I also do a manual smoke testing after each feature is ready to merge. <I haven't experimented with something like playwright yet, but I suspect that would be a better approach to catch UI regressions, where a lot of the complexity of the app resides

*** 5. Don't let Claude debug on its own

Debugging is another interesting one.

For an unsophisticated project like this, with good detail in the prompts, Claude tends to get the implementation right or almost right most of the time---let's say 80%.

I noticed that, when something fails and the problem isn't obvious, Claude can figure out the problem on its own maybe 30% of the times. This can include very subtle or cryptic errors that could take me hours to resolve myself.

The problem is the remaining 70%. I find that the LLM, even with the command line at its disposal, tends to be both clueless and eager to try random things, accumulating layers of useless changes if left unchecked, going in circles and very far from the actual problem.

So what worked for me is to give it one shot to figure out the issue autonomously and, when that fails, take over, not necessarily to implement the fix myself but to feed it plausible hypothesis to test and get it on track to a solution.

*** 6. Don't repeat yourself (but sometimes do)

Code duplication is an interesting thing to reflect about in the context of working with agents. LLMs are paid (?) to output tokens, so unsurprisingly Claude Code indulges in all kinds of duplication, <not only pieces of code already defined elsewhere in the project (and possibly outside of its current context), but pieces defined right above the code its adding, or in a class its sub-classing, in the django built-ins already imported, etc.

It's tempting to add rigid rules to CLAUDE.md to prevent code duplication, but as we collectively learned in the last decade, dogmatically removing all code duplication tends to do more harm than good.

The anniversary edition of /The Pragmatic Programmer/ makes a useful distinction between duplication /of code/ and duplication /of knowledge/. The latter being what we need to strictly remove. <The situation compounds in the context of coding agents, where (re)writing code is effortless but scattered knowledge is the biggest system risk.

So I found most of my reviews deal with strategically removing or allowing duplication: if it's knowledge it needs to be centralized and I need to think carefully how, if it's just code I can instruct how to extract and reuse, but more often than not it's OK just live with that duplication.

[TODO better examples?]

*** 7. Plan around context window and usage limits
- the only thing I found to be fundamental is to follow X advice to turn compaction off. If you reach compaction then that session is dead. And also if you near compaction you'll likely kill any remaining tokens you have in your current session. These constraints made me arrange my work to always be approachable in one session:
  - don't try to do too much at once
  - close and reopen claude as a sort of "checkpointing" when I'm done with a feature and want to start another one.
  - do a plan-only session, with a detailed markdown plan as output for work that seems to complex work on start to finish before running out of context token or session usage limits.

- I find that these restriction to work on self contained sessions, and the need to take breaks when reaching both session and weekly limit made me keep my work organized and avoid the agent subtly dragging me into its spirals of nonsensical coding sprees.

- When I ran out of tokens, since there was still that barrier on familiarizing with the code, I opted to make progress on UX ideas, feature specifications, test stubs and server setup---all pieces that I didn't want the AI to touch. I found this context switching useful to make steady progress.

In a more serious or time-constrained scenario, I would still opt for paying two 20 dollar pro plans instead of a 100 or 200 USD max plan. I haven't tried, but I suspect a combination of Claude Code Pro and Codex Plus is the ideal combination.

*** Conclusions

The process just described may sound like a lot of work and hand-holding, and probably not what the "pros" are doing out there with agents but, as stated before, the goal here was not maximize velocity or throughput but to get a finished product with minimal effort and frustration on my side. I have a fair amount of experience in high-level task breakdown, writing tickets for others to work on, doing superficial code reviews, anticipating pitfalls, and building confidence on a shared codebase through a solid test suite. So this was playing my strengths, and mostly prevented the LLM from digging itself into a hole that I would have to get it out from. I found this micromanaging approach to be very effective and low effort, at times even rewarding---to see features that sounded complicated, and that I would have avoided, work out in a few shots, was stimulating. I occasionally surrendered to the illusion that I was wielding this powerful tool, one that extended my reach and abstracted unimportant details away.


- previous attempt I qualified the feeling as "exhilarating recklessness" and compared the experience with going to the casino.
- this time instead I felt like I was leveraging my experience: like I had been saving in experience for 15 years and Claude allowed me to "spend" some of that experience to get something that I wanted. This analogy works in more than one way:
  - it felt rewarding to see how much I know about this stuff that I had not think about in a long while, how many little nuances about putting together a web app and trying different user interfaces I could think about, how many problems I could catch by just reviewing Claude's code. Even if wasn't writing the code, there was a lot I could bring to the table .
  - I did feel like if I only coded this way my experience would give diminishing returns and my skills would atrophy

- I'm sure there are many mistakes I made by letting claude do the work for me, but this was clearly a successful project given my goals and the results
- I still don't think it would be wise to use this a lot in professional environments, other than for proof of concept-type of software. I don't think that trading short term productivity for long term ownership and knowledge building makes sense in most cases.

*** Notes

[fn:3] Using AI was not in itself a project goal, though. I first made my mind about spending time on this project; once that was settled, I tried Claude Code as an experiment. Had I had a first few unproductive sessions I would have started over writing all the code myself (perhaps running a higher risk of abandoning the project before down the line).
[fn:1] This is not an original idea: there's already a similar application that worked pretty well for a few years, until they introduced an aggressive subscription model. This change stagnated the community---both for free and paid users---, so the motivation for my project was to provide a free replacement to that community.
