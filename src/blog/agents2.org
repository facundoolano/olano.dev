---
title: My first win with agents
date: 2025-12-13T11:59:38-03:00
layout: post
lang: en
tags: [ai, projects]
draft: true
excerpt: "I built a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?"
image: giralibros.png
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

Previously on /Facundo's Adventures with LLMs/:

- [[https://jorge.olano.dev/blog/on-ai-assistance/][On AI assistance]] (2024-03): I start to occasionally reach to ChatGPT while working on a new open-source project, especially for chores like "I wish there already was a lib for this", "I need a Makefile recipe for this", etc. I see the potential; the results are hit-and-miss. I make plans to better integrate this tech into my workflow.
- [[augmentation-replacement][Augmentation / Replacement]] (2025-03): I added [[https://github.com/karthink/gptel][gptel]] to my Emacs setup and see myself using LLMs daily, as a rubber duck and Google's L1 Cache. I recognize the human augmentation potential of AI---not so much for what LLMs are today, but for the glimpse we get of what a better AI could be. On the other hand, the current push to use LLMs as human replacements seems short-sighted and counter-productive.
- [[agentic-coding-experience][Quick notes on a brief agentic coding experience]] (2025-06): I naively try to use Claude Code to implement some features of an already working web application, burn some cash in the process, and get nothing much out of it.

This time I will document a successful experience I had building a small web app almost exclusively with Claude Code. My previous attempt at coding with agents had made me sick, but this time I felt empowered. What changed?

** Part I: the project
The project is a small book trading web application for the Buenos Aires local area.

#+BEGIN_EXPORT html
<div class="text-center">
<img src="{{site.config.static_root}}/img/giralibros.png"></a>
</div>
#+END_EXPORT


1. Users publish a list of books they offer for trading, and browse other user's books.
2. When they see something they like, the user sends an exchange request to the book owner, who receives it as an email notification.
3. If the owner is interested in any of the other user's books, they arrange to meet and make the trade. This exchange takes place outside of the application (there are no incentives to keep users in-app; email and WhatsApp are more convenient).

I wrote this application from scratch[fn:1] using Django, SQLite for the database, and Bulma for styles. It runs on a small Debian server behind nginx.

*** Goals
- *Finishing the project*, with the exact UX I had in mind (which was very simple).
- *Minimizing the effort* I had to make to implement it (counting the frustration and disgust with the tooling, e.g. Claude Code, as part of that effort).
- *Minimizing operational costs* to run the system: if this was successful I would run it as a free community project, so I needed to design it to be cheap to run and not take much of my time.
- *Keeping a decent understanding of the codebase* (at least of its backend).

*** Non-goals
There are a number of things I typically go for in personal projects, which I explicitly didn't care about for this one:
- Maximizing speed: as long as I got it finished with low effort, I was in no rush.
- Having fun.
- Learning.
- Ensuring long-term maintainability, flexibility, or extensibility of the codebase: in a way this was a proof of concept. I wanted to get it out there and see if people liked it. It's small enough that I can risk not having a strong hold of the codebase, because it wouldn't be hard to quickly rewrite it if necessary.
- Building a successful product: I want this to succeed, not least because I want to use it to trade books but, other than making the application free and accessible, it's out of my hands whether people choose to use it or not---I'm not going to go out of my way to promote it, either.
- Keeping users happy: since this is not a business, I can afford to miss features, ship bugs, lose data, etc. I got nothing to lose.

While all the things on this second list are desirable, I was willing to trade them off for those on the first one.

*** Using agents
Given the specific mix of goals and non-goals, this seemed like a opportunity to have another try at
building with agents, instead of writing all the code myself[fn:3]. I could afford taking some of the risks I associate with delegating too much to the AI---shipping something I don't fully understand, that could take me an extra effort to fix when it breaks, or that turns out to be unmaintainable in the long run.

A few things had changed since my previous previous experiment with Claude Code:
- One of the reasons I didn't keep up with Claude Code was that a 100-200 USD/month subscription seemed too much for my occasional side projects, and the API key alternative could easily burn 30 bucks in a day. This time I switched to the more reasonable 20 USD/month Pro plan.
- During this time, I kept reading accounts of agent coding experiences from other (often skeptical) devs, which gave me new ideas and better context to work with these tools.
- Probably the biggest difference, this was going to be a greenfield project, which is where agents shine. The stack was more LLM-friendly too, I believe: the Django guardrails plus vanilla JavaScript are a better match to their training set than Flask, SQLAlchemy and the HTMX/hyperscript extravaganza from my other project.

Django was an ideal fit for me to leverage LLMs: I used Django intensively for 5 years... a decade ago. I still have a good grasp of its concepts, a notion of what's doable with it, what's built-in and what should be ad hoc, etc., but I completely forgot the ORM syntax, the class names, the little details. I could instruct Claude exactly what to do, saving me a lot of documentation roundtrips while still catching whenever it tried to bullshit me into getting creative or reinventing the wheel.

For the front-end the risk/reward was a bit higher. I've been officially a backend dev for a while now,
and while I've used Bulma on a few projects and have a good idea of what it offers, I'm not trained to review HTML and CSS, so it's likelier that Claude would slip working, superficially good-looking front end code that would quickly degrade and become an unmaintainable mess. On the other hand, and for the same reasons, despite my best intentions the HTML and CSS I produce manually tends to be less maintainable anyway---Claude would just accelerate the process. This turned out to be a very good trade off, since Claude allowed me to iterate quickly in prototype mode, to arrive to the look-and-feel that worked well for the project, something that would have taken me a big effort if I was writing the HTML myself.

*** Results
I released an MVP after one week of part-time work, and a few extra nice to haves in another week. At the time of writing, there are ~80 registered users and ~400 offered books. The app doesn't track this, but I know first hand of a few book exchanges that already took place.

The costs of running the app are:
  - 6 USD/month on a Hetzner VPS
  - 7 USD/year on a Porkbun domain
  - 2.5 USD/every 6 months for ZeptoMail credits

The 7 USD/month total is about the cost of a second hand book here.

** Part II: the process
I really liked the recent [[https://checkeagle.com/checklists/njr/a-month-of-chat-oriented-programming/][/A Month of Chat-Oriented Programming/]] post and I borrowed a few ideas from it. I like the notion of chat-oriented programming as opposed to vibe-coding. That's what I tried to do with this project, although my own variation of it.

*** 1. Skip agent setup
I made a very deliberate choice not to invest in agent customization and support tooling, or whatever that's called: no fancy CLAUDE.md instructions, no MCP servers, no custom commands, no system prompts, no skills, no plugins. I'm not saying these aren't useful, but I find them to be non-conducive rabbit holes: in my previous experiments I ended up spending a lot of tokens trying to come up with an ideal workflow specification, which claude would still randomly forget or ignore.

A fundamental problem is that the agent doesn't seem to have much knowledge about itself, its configuration, or its commands. When deep in conversational mode, one feels inclined to approach tweaking the tool in the same way, asking the tool to explain how to best use it or telling it to adjust a specific configuration, only to find it has no clue about how to do that. In that context, tweaking the agent at will requires a big context switch; in my opinion, spending mental cycles in such meta-tasks defeats the purpose of coding with agents. Since these tools are changing every month, I'm not compelled to make an investment unless it yields immediate results. My attitude then was: see how far this can get me today with minimal tweaking; if that's not very far, I'll just wait and try again in a few months.

*** 2. Switch tactically between default, edit, and plan mode
For non trivial features I started the session with some product-level context of the requirement, and the subset of files relevant to the implementation. Then I bounced some ideas with Claude, sometimes providing a succinct TODO list of things that I expected to change in the app or the code. My goal was to minimize the opportunities for it to get creative or improvise, while still giving it room to catch weak spots in my reasoning.

For the few more involved features I worked exclusively in plan mode, having Claude Code produce a markdown file as the output of the session, to be picked up on a separate session.

*** 3. Claude writes all the code, but never commits
I reviewed all code before each commit, asking for refactors of the obvious mistakes, and again before merging the PR. This process pushed me to break the tasks down into obvious increments that would make good checkpoints for committing, which I made part of my instructions (this is about the same I do when I'm writing the code). When Claude was working, I kept an eye on the console output and interrupted it when it looked like it was trying to do too much at once.

*** 4. The human decides what and how to test
I find that an explicit test exercising every relevant "business rule" is more effective than documentation, code comments, and the overall design/architecture in capturing the desired system behavior and guaranteeing that it  does what it is supposed to do. This is even more important in the context of agentic coding, where I'm voluntarily resigning some control over the implementation.

I mostly agree with the sacred rule in [[https://diwank.space/field-notes-from-shipping-real-code-with-claude][/Field Notes From Shipping Real Code With Claude/]]: /Never. Let. AI. Write. Your. Tests./ I was slightly less strict, though: instead of writing the tests myself, I provided a set of rules and step-by-step outlines of the integration tests I wanted:

#+begin_src python
def request_book_exchange(self):
    # register two users
    # first user with 3 books
    # second user two books
    # send request for second book
    # check outgoing email
    # check email content includes 2nd user contact details
    # check email content lists user books
    pass

def mark_as_already_requested(self):
    # register two users
    # first user with 3 books
    # second user gets home, sees all three books and Change button
    # send request for second book
    # request list shows 2 Change, one already request
    pass
#+end_src

Then I carefully reviewed the implementation to ensure it followed my testing preferences: don't couple to implementation details (test units of behavior, not units of code), don't mock intermediate layers (just the inputs and outputs of your system, i.e. its observable behavior), don't access the DB directly[fn:2]. Once a few tests were in place, Claude was less likely to deviate from the surrounding style.

I also did some smoke testing after each feature was ready to merge. I haven't experimented with something like Playwright, but I suspect that would be a good addition to prevent regressions in the UI, which is where most of the application complexity resides.

*** 5. Don't let Claude debug on its own

For an unsophisticated project like this, with good enough detail in the prompts, Claude tends to get the implementation right or almost right let's say 80% of the time.

I noticed that, when something fails and the problem isn't obvious, Claude can figure out the problem on its own maybe 30% of the times. This can include very subtle or cryptic errors that could take me hours to resolve myself. The problem is that remaining 70%. I find that the LLM, even with the command line at its disposal, if left unchecked will  be both clueless and eager to try things at random, accumulating layers of useless changes, going in circles and very far from the actual problem.

What worked for me is to give it one shot to figure out the issue autonomously and, when that fails, take over, not necessarily to do the full debugging and fix myself but to feed it plausible hypotheses and evidence, to put it back on track to a solution.

*** 6. Don't repeat yourself (but sometimes do)

Code duplication is an interesting thing to reflect about in the context of working with agents. LLMs get paid (?) to output tokens, so unsurprisingly Claude Code indulges in all kinds of duplication, from repeating patterns found in the same module to reimplementing entire chunks of the Django built-ins its already importing. It would be tempting to add strict rules to CLAUDE.md to prevent all code duplication, but as we collectively learned in the last decade, dogmatically applying DRY tends to do more harm than good.

The anniversary edition of /The Pragmatic Programmer/ makes a useful distinction between duplication /of code/ and duplication /of knowledge/, the latter being what we need to be more wary of. In the context of coding with LLMs, where (re)writing code is effortless but scattered knowledge is a threat to system survival,
the distinction is fundamental. So I found most of my reviews deal with strategically removing or allowing duplication: if it's knowledge, it needs to be centralized and I need to think carefully how; if it's just code, I can instruct how to extract and reuse, but more often than not it's better just live with that duplication.

*** 7. Plan around token and session limits

There are a few usage limits that need to be taken into account when working with Claude Code.

The first is the size of the conversation context window. There is so much context the model can fit when processing a new message, and a long running session will eventually exhaust it. By default, CC will try to "compact" the context to keep it manageable but, as noted in the /Month of CHOP/ post, this degrades the quality of its output. I also found that the process of compaction itself spends a lot of tokens, which is problematic because of the Plan usage limits.

I followed /Month of CHOP/ post advice to turn off autocompaction in the settings, and kept an eye on its token consumption via the ~/context~ command, which looks like this:
#+begin_src
> /context
  ⎿
  ⎿   Context Usage
  ⎿  ⛁ ⛀ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛀   claude-sonnet-4-5-20250929 · 80k/200k tokens (40%)
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁   ⛁ System prompt: 3.1k tokens (1.6%)
  ⎿  ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁ ⛁   ⛁ System tools: 15.3k tokens (7.7%)
  ⎿  ⛁ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛁ Memory files: 1.8k tokens (0.9%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛁ Messages: 60.0k tokens (30.0%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶   ⛶ Free space: 120k (59.8%)
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿  ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶ ⛶
  ⎿
  ⎿  Memory files · /memory
  ⎿  └ Project (/Users/facundo/dev/facundoolano/giralibros/CLAUDE.md): 1.8k tokens
  ⎿
  ⎿  SlashCommand Tool · 1 commands
  ⎿  └ Total: 981 tokens
  ⎿
  ⎿
#+end_src

In addition to the model context window, Claude Pro has limits on how much you can consume in session and weekly time windows, which can be monitored at [[claude.ai/settings/usage][https://claude.ai/settings/usage]].

#+BEGIN_EXPORT html
<div class="text-center">
<img src="{{site.config.static_root}}/img/claudeplan.png"></a>
</div>
#+END_EXPORT


I've observed that I exhaust the session limit after a couple of hours of steady work (having to wait 3-5 hours to resume), and a weekly plan limit if I work ~4-5 days in a row, so I learned to plan my work around these constraints.

- Much like I split coding tasks into commit-sized steps, I split features into PR-sized sessions.
- I tried to work on a single thing at a time, e.g. resisting the temptation to add extra/unrelated tasks as they popped to mind. Which was a good idea, anyway, both for project organization and to keep the agent focused on the task at hand.
- I monitored the context window, restarting Claude to "checkpoint" when I was done with a feature and wanted to start on another.
- For bigger tasks that I anticipated wouldn't fit in the context window or session limit, I would do one or more planning sessions first, with a detailed markdown plan as output that could be picked up on a subsequent implementation session.

While it was very annoying to hit these limits at first, I think it eventually pushed me to stay methodical.
If I ran out of tokens and I wanted to continue working in the project, I would switch to non-AI driven activities (plan, research, stub tests, server config, etc.). I find this was a healthy balance for me.

If this was my job and not a side project, and I wanted to improve my throughput, rather than switching to a 100 USD Max plan, I would combine this Pro plan with Codex Plus from OpenAI (also 20USD/month, and would give me exposure to a different model).

*** Conclusions

The process just described may sound like a lot of work and hand-holding, and probably not what the "pros" are doing out there with agents but, as stated before, the goal here was not maximize velocity or throughput but to get a finished product with minimal effort and frustration on my side. I have a fair amount of experience in high-level task breakdown, writing tickets for others to work on, doing superficial code reviews, anticipating pitfalls, and building confidence on a shared codebase through a solid test suite. So this was playing my strengths, and mostly prevented the LLM from digging itself into a hole that I would have to get it out from. I found this micromanaging approach to be very effective and low effort, at times even rewarding---to see features that sounded complicated, and that I would have avoided, work out in a few shots, was stimulating. I occasionally surrendered to the illusion that I was wielding this powerful tool, one that extended my reach and abstracted unimportant details away.


- previous attempt I qualified the feeling as "exhilarating recklessness" and compared the experience with going to the casino.
- this time instead I felt like I was leveraging my experience: like I had been saving in experience for 15 years and Claude allowed me to "spend" some of that experience to get something that I wanted. This analogy works in more than one way:
  - it felt rewarding to see how much I know about this stuff that I had not think about in a long while, how many little nuances about putting together a web app and trying different user interfaces I could think about, how many problems I could catch by just reviewing Claude's code. Even if wasn't writing the code, there was a lot I could bring to the table .
  - I did feel like if I only coded this way my experience would give diminishing returns and my skills would atrophy

- I'm sure there are many mistakes I made by letting claude do the work for me, but this was clearly a successful project given my goals and the results
- I still don't think it would be wise to use this a lot in professional environments, other than for proof of concept-type of software. I don't think that trading short term productivity for long term ownership and knowledge building makes sense in most cases.

*** Notes
[fn:2] My testing rules are listed in [LINK] and [LINK]. I asked claude to read them and dump a summary to CLAUDE.md, but I doubt that made much of a difference.

[fn:3] Using AI was not in itself a project goal, though. I first made my mind about spending time on this project; once that was settled, I tried Claude Code as an experiment. Had I had a first few unproductive sessions I would have started over writing all the code myself (perhaps running a higher risk of abandoning the project before down the line).
[fn:1] This is not an original idea: there's already a similar application that worked pretty well for a few years, until they introduced an aggressive subscription model. This change stagnated the community---both for free and paid users---, so the motivation for my project was to provide a free replacement to that community.
