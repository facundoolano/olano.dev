---
title: Quick notes on a brief agentic coding experience
date: 2025-06-16 14:50:19
layout: post
lang: en
tags: [ai]
draft: true
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

1. Perhaps inadvisable from a mental health perspective,  I have my RSS reader set up to fetch top stories from Hacker News and lobste.rs. This means every morning I get a fresh batch of AI-related blog posts delivered to my feed.
2. I continue to feel ambiguous about this topic and I continue to enjoy reading all but the maximalist takes on either side of the fence. I like to see how people are trying and failing to benefit from these tools, and how they invest in their setup and get something remarkable out of them. I particularly liked a couple of recent stories: [[https://diwank.space/field-notes-from-shipping-real-code-with-claude][Field Notes From Shipping Real Code With Claude]] and [[https://lucumr.pocoo.org/2025/6/12/agentic-coding/][Agentic Coding Recommendations]].
3. What I liked about these is that the authors don't deny the limitations of agents, they don't pretend that everything works magically out of the box, and they give practical recommendations on how to best use them. I especially appreciate the "never let AI write your tests" rule from the first link. I imagine it's not the only way to go about it, but it resonates with my own pre-AI experience. Making a big investment in getting the tests just right, so they become insurance against untrustworthy code, I something I relate to. I can see Agentic coding as an exacerbation of this scenario.
4. Before this I had been using LLMs almost exclusively in chat sessions inside Emacs, through [[https://github.com/karthink/gptel][gptel]]. First ChatGPT, then Claude. After reading the linked stories I decided to give [[https://www.anthropic.com/claude-code][Claude Code]] a try, using my personal [[https://github.com/facundoolano/feedi][feed reader]] as a playground. I planned to task the LLM with a bunch of little bug fixes, refactors, and features I'd been filing since I started to use the app, but that I hadn't planned to tackle any time soon. This is the resulting raw, stream-of-conscious bullet-list brain dump of that process.
5. I can summarize my experience like this: exhilarating and irresponsible. Irreponsibly exhilarating? exhilaratingly irresponsible? Kind of like going to the casino: addictively fun, but you can lose your house if you don't pay attention.
6. Just like with my earlier experiences with LLMs, it's the interface that I'm most impressed about. In this case, it's how the tool can interact with shell commands, how it runs little experiments to improve it's context, how I can conversationally help it learn new tricks and adapt its workflow to my preferences. Claude sonnet is just as knowledgeable and dumb and sloppy as in the chat buffer, but the fact that it can access my project, do trial an error on its own, and rely on any tool I give it access to, makes up for a good chunk of its limitations.
7. I can't really say that Claude Code made me more productive in terms of time, let alone quality, that if I was tackling the same tasks manually in my editor. I mean if I really had to, if this was real work. But the thing is: this was a side project, not real work, and these were features that I had been putting off, and it was the fun I had with the agent what pushed me to get them done. What's most important is that the part of my brain that I needed to engage in the process was completely different: I didn't need to concentrate nearly as much, even while I was carefully reviewing all the code, I wouldn't get as tired, and it didn't feel like working on my free time.
8. I must clarify that I wasn't /vibecoding/, I'm not interested in entirely letting go of my codebase like that. I wanted to provide instructions and let the agent do its thing, but then I would review and iterate on the changes before merging them. Because I had expectations to continue using the app, I wanted to be able to still understand and extend the code myself when necessary, and the signal I was getting was that I couldn't really trust the agent to do the reasonable thing most of the time.
9. It needs /a lot/ of babysitting and guard railing, which is kind of what I expected going in. I could perhaps improve the experience if I was willing to make a bigger investment in the process---carefully curating my CLAUDE.md, writing better tools, tuning my workflow. By which I'd be surely out of magic productivity land, onto deliberate tool building. Which would be fine, it would be worthwhile and even fun, if it wasn't that this thing is so ridiculously expensive.
10. Because the elephant in the room, the showstopper problem, was the amount of money I needed to pour into it to keep going. I worked for a couple of half-day sessions, maybe 4-6 hours in total, and spent about 30 dollars to get a couple of trivial code edits and a half-broken simple feature.
11. Some people would say this is cheap if you compare it to a typical programmer's salary instead of a typical software subscription, a comparison I'm not willing to make. And maybe the monthly subscription is more cost-effective than the pay-as-you-go model---I didn't really do the math. But I live in the Third World and I've learned to be weary of where I stick my credit card numbers. This is not a work tool I'm paying for; it's a dopamine fix. Like a slot machine, I was pouring money in one end just to see what came out on the other, just to see if I'd get lucky. It was fun but also made me sick and felt wrong.
12. Ultimately, this mode of working felt irresponsible because I could witness, real-time, how I was being estranged from my own project. I've seen this before, in one of my open-source projects: after a few years I lost interest in it and let the community take over, limiting myself to reviewing external contributions. Since I didn't use it myself, and didn't care much about it, I would just accept most seemingly working contributions. Until the codebase changed so much that I couldn't even understand it's general structure. By now I couldn't update it without a big re-learning effort. Seeing the LLM agent do it's thing showed my how that process could be accelerated, from several years to a week or two.
13. There's more to it. You can always familiarize with a strange project, no matter how badly shaped it is. I cleared a few [[https://increment.com/software-architecture/exit-the-haunted-forest/][haunted-forests]] in my career---progressively understanding and recovering ownership of a project, rewriting it line-by-line without indulging in the lazy start-over---and my experience is: for all of the mud that can accumulate over years of careless maintenance, if you know how to look, you can always find traces of intent, you can infer the presence, the needs and constraints of previous maintainers. And that you can use to put back parts of the puzzle together, to gain understanding and confidence to support some of your decisions. This wouldn't be the case, I believe, with LLM-generated code. With LLMs it's all  random text generation, plausible nonsense, mocked intent. Past certain threshold, the surrender would become irreversible: there's no resurrection for that kind of project death.
14. If Claude Code was cheaper I think I could have a good kick out of it for certain low-stakes projects. No work, all play. I don't think I could bring myself to program like this professionally, though. At least the current state of the art feels incompatible with my duties as a software designer.
