---
title: From RSS to my Kindle
subtitle: Building website epubs with Python
date: 2024-06-01 20:24:33
layout: post
lang: en
tags: [web, projects, programming]
draft: true
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

Last year I wrote about [[file:reclaiming-the-web-with-a-personal-reader][how I built a personal feed reader]] and started using it as my front page to the web. In the months that followed I continued to tweak the app, observing my reading habits, experimenting with new features and discarding the ones I wasn't using. I now got it to a place where I can count on seeing fresh and interesting content a couple of times a day, and the interface conveniently lets me keep what I plan to read and discard the rest.

So I get dozens of interesting articles and blog posts every week but, while I'm an otherwise avid reader, I struggle to read on a monitor (let alone on a phone screen) without losing concentration or getting eye strain. Whenever I see a longer piece that seems interesting but I don't have the disposition to read /right now/, I prefer to send it to my Kindle for reading later ---much like other people use Pocket or just collect browser tabs. I can then go back to it  offline, whether I'm in bed, in the bathroom, at a cafe or on the bus.

So a Kindle integration was a natural extension to my feed reader. Not only because it would streamline my workflow: because the official Amazon Send To Kindle extension only works on Chrome and does a poor job of extracting the HTML content from most websites. I was already using Mozilla's [[https://github.com/mozilla/readability][readability library]] to embed article content in my app, so I just needed to figure out how to send that content to my Kindle. I had to learn some subtleties along the way, so I decided to document the implementation process.

-----

- first implementation using hacky lib. this had a number of problems

- at a high level, this is what the web app route looks like:
#+begin_src python
# feedi/routes.py

@app.post("/entries/kindle")
def send_to_kindle():
    url = flask.request.args['url']
    article = scraping.extract(url)
    attach_data = scraping.package_epub(url, article)
    email.send(current_user.kindle_email, attach_data, filename=article['title'])
    return '', 204
#+end_src



- first readability nodejs script
#+begin_src javascript
#!/usr/bin/env node

const { JSDOM } = require("jsdom");
const { Readability } = require('@mozilla/readability');

const url = process.argv[2];

JSDOM.fromURL(url).then(function (dom) {
  let reader = new Readability(dom.window.document);
  let article = reader.parse();
  process.stdout.write(JSON.stringify(article), process.exit);
});
#+end_src

the output looks like this:
#+begin_src json
TODO
#+end_src

- how it's called from python
#+begin_src python
# feedi/scraping.py
import json
import subprocess

def extract(url):
    r = subprocess.run(["feedi/extract_article.js", url],
                       capture_output=True, text=True, check=True)

    article = json.loads(r.stdout)
    return article
#+end_src

- since some sites do lazy loading of images:

#+begin_src diff

import json
import subprocess

+ from bs4 import BeautifulSoup

def extract_article(url):
    r = subprocess.run(["feedi/extract_article.js", url],
                       capture_output=True, text=True, check=True)

    article = json.loads(r.stdout)

+    # load lazy images by setting data-src into src
+    soup = BeautifulSoup(article['content'], 'lxml')
+    LAZY_DATA_ATTRS = ['data-src', 'data-lazy-src', 'data-srcset',
+                       'data-td-src-property']
+    for data_attr in LAZY_DATA_ATTRS:
+        for img in soup.findAll('img', attrs={data_attr: True}):
+            img.attrs = {'src': img[data_attr]}
+
+    article['content'] = str(soup)

    return article

#+end_src

- the bulk of the work: package the extracted html as an epub
  - prepare a zip, since an epub is a zipped webpage with some assets and metadata files. write to a file inside the zip, output as bytes. naive implementation:
#+begin_src python
# feedi/scraping.py
import io
import zipfile

def package_epub(url, article):
    output_buffer = io.BytesIO()
    with zipfile.ZipFile(output_buffer, 'w', compression=zipfile.ZIP_DEFLATED) as zip:
        zip.writestr('article.html', article['content'])

    return output_buffer.getvalue()
#+end_src

  - write some epub metadata, based on https://github.com/thansen0/sample-epub-minimal
#+begin_src  python
zip.writestr('mimetype', "application/epub+zip")
zip.writestr('META-INF/container.xml', """<?xml version="1.0"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
<rootfiles>
<rootfile full-path="content.opf" media-type="application/oebps-package+xml"/>
</rootfiles>
</container>""")

author = article['byline'] or article['siteName']
if not author:
    # if no explicit author in the website, use the domain
    author = urllib.parse.urlparse(url).netloc.replace('www.', '')

zip.writestr('content.opf', f"""<?xml version="1.0" encoding="UTF-8"?>
<package xmlns="http://www.idpf.org/2007/opf" version="3.0" xml:lang="en" unique-identifier="uid" prefix="cc: http://creativecommons.org/ns#">
<metadata xmlns:dc="http://purl.org/dc/elements/1.1/">
<dc:title id="title">{article['title']}</dc:title>
<dc:creator>{author}</dc:creator>
<dc:language>{article.get('lang', '')}</dc:language>
<dc:date>{article.get('publishedTime', '')}</dc:date>
</metadata>
<manifest>
<item id="article" href="article.html" media-type="text/html" />
</manifest>
<spine toc="ncx">
<itemref idref="article" />
</spine>
</package>""")
#+end_src

  - go through the images in the article, rewrite the src to point to local images instead of remote ones, download the images
#+begin_src diff
 import io
 import zipfile

+from bs4 import BeautifulSoup

 def package_epub(url, article):
     output_buffer = io.BytesIO()
     with zipfile.ZipFile(output_buffer, 'w', compression=zipfile.ZIP_DEFLATED) as zip:
-        zip.writestr('article.html', article['content'])
+        soup = BeautifulSoup(article['content'], 'lxml')
+        for img in soup.findAll('img'):
+            img_url = img['src']
+            img_filename = 'article_files/' + img['src'].split('/')[-1].split('?')[0]
+
+            # update each img src url to point to the local copy of the file
+            img['src'] = img_filename
+
+            # download the image and save into the files subdir of the zip
+            response = requests.get(img_url)
+            if not response.ok:
+                continue
+            zip.writestr(img_filename, response.content)
+
+        zip.writestr('article.html', str(soup))
     return output_buffer.getvalue()
#+end_src
  - if webp, convert it to jpg, since kindle doesn't support webp

#+begin_src diff
 import io
 import zipfile

 from bs4 import BeautifulSoup
+from PIL import Image

 def package_epub(url, article):
     output_buffer = io.BytesIO()
     with zipfile.ZipFile(output_buffer, 'w', compression=zipfile.ZIP_DEFLATED) as zip:
         soup = BeautifulSoup(article['content'], 'lxml')
         for img in soup.findAll('img'):
             img_url = img['src']
             img_filename = 'article_files/' + img['src'].split('/')[-1].split('?')[0]
+            img_filename = img_filename.replace('.webp', '.jpg')

             # update each img src url to point to the local copy of the file
             img['src'] = img_filename

             # download the image and save into the files subdir of the zip
             response = requests.get(img_url)
             if not response.ok:
                 continue

-            zip.writestr(img_filename, response.content)
+            with zip.open(img_filename, 'w') as dest_file:
+                if img_url.endswith('.webp'):
+                    jpg_img = Image.open(io.BytesIO(response.content)).convert("RGB")
+                    jpg_img.save(dest_file, "JPEG")
+                else:
+                    dest_file.write(response.content)

         zip.writestr('article.html', str(soup))
#+end_src

- didn't want to integrate with a mailing service, so I looked into using a regular gmail account, which I remember (from a decade ago, when I did django freelancing) was a viable option to send small amounts of emails.
  - things were slightly different: I couldn't just use the email password in my app. I needed a new thing called "app passwords" and to generate that I needed to setup 2fa first. Then I had to find an obscure page to get the app password
#+begin_src python
# feedi/email.py
import smtplib
import urllib.parse
from email import encoders
from email.mime.base import MIMEBase
from email.mime.multipart import MIMEMultipart

def send(recipient, attach_data, filename):
    server = "smtp.gmail.com"
    port = 587
    sender = "my.reader.email@gmail.com"
    password = "some gmail app pass"

    msg = MIMEMultipart()
    msg['From'] = sender
    msg['To'] = recipient
    msg['Subject'] = f'feedi - {filename}'
#+end_src
  - attach epub bytes
  - set the title as filename. important since this is what kindle displays. Since that would typically include spaces and special characters for non-english languages, I needed to use a weird escaping syntax see https://stackoverflow.com/a/216777/993769
#+begin_src  python
part = MIMEBase('application', 'epub')
part.set_payload(attach_data)
encoders.encode_base64(part)

filename = urllib.parse.quote(filename)
part.add_header('Content-Disposition', f"attachment; filename*=UTF-8''{filename}.epub")
msg.attach(part)
#+end_src

  - finally, I sent the email like this:
#+begin_src python
smtp = smtplib.SMTP(server, port)
smtp.ehlo()
smtp.starttls()
smtp.login(sender, password)
smtp.sendmail(sender, recipient, msg.as_string())
smtp.quit()
#+end_src

- for this to work, of course, I also had to go to my amazon device settings and approve my reader email as a personal document email.

- this is working well enough, but there are a few things I wish I had
  - support for js dependent content. I experimented with headless browsers early on, but found it ultimately made the parsing more brittle so I decided to stick with static html content
  - ff extension to send articles outside feedi, through feedi
  - share target, which unfortunately doesn't work for progressive web apps in iOS. the alternative of building a safari extension is expensive and way too much work
