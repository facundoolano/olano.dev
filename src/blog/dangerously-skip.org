---
title: "-â€‹-dangerously-skip-reading-code"
date: 2026-02-14T21:17:15-03:00
layout: post
lang: en
tags: [ai, software]
draft: true
image: bird.png
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

I concluded my [[tactical-tornado][previous post]] saying it was irresponsible to ignore the downsides of delegating to LLMs  under the assumption that we won't need to worry anymore about reading and debugging our code---that whatever problem that pops up the LLMs will fix for us. This felt irresponsible because, up until now, it has been the programmer's job to understand and maintain the source code, as a proxy to understanding and maintaining the system. We are held accountable for the LLM's output.

But what if this wasn't the case anymore? What if we dutifully communicate the risks and trade-offs to our organizational leadership and they still want to take those risks? This isn't unheard of: companies, and especially tech startups, regularly make short-term compromises to improve productivity, beat the competition to market, lure investors, etc.

If there's an organizational mandate to leverage LLMs to minimize time spent coding, then that's a new constraint we can work with. We can figure out what good engineering looks like in that context. We can stop reading LLM-generated code just like we don't read assembly, or bytecode, or transpiled JavaScript; our high-level programming source code is now a new form of machine code.

This finally clicked for me after reading [[id:][Thougtworks' retreat report]]. The LLMs produce non-deterministic output and generate code much faster than we can read it, so we can't seriously expect to effectively review, understand, and approve every line of code anymore. But that doesn't necessarily mean we stop being rigorous, it means we need to move the rigor elsewhere.

It's fundamental to understand, though, that this is not an individual's or team's call: it has to be an organizational decision, and not just because of risk management and accountability: because of Amdahl's law. If we only maximize code generation speed without rearranging the processes and organizational structures in which our work is embedded, there won't be any tangible productivity gains.

We can't have some devs pumping 20k lines of slop a day and expect the rest to still read and understand it, let alone approve it. We can't leverage agents if our unit of work is still "add this RESTful API endpoint". We can't expect a Product Owner to stream enough requirements to keep a two-pizza team busy if they can work on 4 tasks at a time and keep agents running off-hours.

We need a virtually infinite supply of requirements, engineers acting as pseudo product designers, owning entire streams of work, with purview to make unconsulted decisions. We need to reduce humans in the loop, friction, bureaucracy, and gate-keeping. Rework is almost free so we shouldn't make an effort to prevent incorrect work from happening.

Then where does the rigor go? In the same lines as the Thoughtworks report, my first be would be on specifications (which are not the same as prompts) and tests (which is not the same as TDD). If I had to roll out such development process today, I'd make a standardized markdown specification the new unit of knowledge for the software project. Product owners and engineers could initially collaborate on this spec and the high-level test cases to enforce it. Those should be checked into the project repositories as part of pull-requests. There would need to be LLM bots checking the PR diffs and the resulting branch to verify that the code conforms the specification. And then this specification, updated as needed during development, would be the target of peer reviews. The specification, and not the code that materializes it, is what the team would need to understand, review, and be held accountable for.
