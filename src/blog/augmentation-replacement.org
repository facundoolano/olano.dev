---
title: Augmentation / Replacement
date: 2025-05-01 13:40:49
layout: post
lang: en
tags: [ai, software, programming]
draft: true
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

I settled on a convenient way[fn:7] to integrate LLMs to my workflow and I've been using them increasingly for my job, for personal projects, and for writing. It can fill several roles for me:

- a sophisticated search engine with the most flexible of query languages and arbitrarily customizable output format[fn:4];
- the ultimate rubber duck, one that can spit back statistically generated opinions;
- a fast, knowledgeable, overeager, and sloppy junior programmer[fn:5];
- an English proofreader with a bad taste.

But beyond these concrete use cases, I see much potential in its flexibility. I unlocked some of that extra ambition Simon Willison keeps talking about: that of tackling projects that I would otherwise skip not because I couldn't do them but because they wouldn't be worth the effort[fn:2]. As the technology and the tooling continue to improve, I can see myself building more sophisticated workflows and personal applications; I can see myself using voice instructions for pair programming, letting it run commands in the terminal; relying rely less and less on google and fact checking; building local knowledge bases with all my code repositories and my writings and any note I want to scribble in the margin of my editor, a second brain to query and look at from all angles.

I don't tend to get excited about technology in and of itself, and LLMs aren't the exception, but if I filter all the noise---which is loud---and try to be objective, I acknowledge that even in their present flawed incarnation they may well be the most powerful human augmentation tool in the history of computing[fn:1]. But for all this current and future power, I'm convinced that generative AI will continue to be a mediocre human replacement when it comes to intellectual and creative work[fn:6].

#+BEGIN_CENTER
\lowast{}
#+END_CENTER

1. non technical folks tend to think that software engineering is limited to writing working code
    software engineering is writing code, so an engineer is a slow and expensive alternative to an LLM

#+BEGIN_CENTER
\lowast{}
#+END_CENTER

I'm thrown off when the ones preaching <> are bright software engineers.
It makes me wonder how different their day-to-day must be from mine for them to reach that conclusion.

The first obvious note is that, as a senior software engineer, it's been a while since most of my day was about just writing code. So it's hard to buy the idea that we could unlock high productivity boosts by letting the machines write the code while we do something else (even if there weren't downsides to that). But ignoring that fact for now and looking specifically at the tasks that involve code writing, I can see three typical /modes/ in which I operate:

1. <trivial amount of code changes, 1-10 lines, preceeded by fact checking, looking at the code, the logs, the data in the db, analytics, talking with the product owner, considering unforseen corner cases, ambiguities, conflicts with preexisting functionality.
2. Meaty coding tasks where I first spend a fair amount of time thinking, doodling and making lists of what I need to do. 90% of the effort is in that stage, writing the code is like the prize. By the time I know enough that I could write a precise prompt, most of the work would already be done. If I had a problem with typing, I guess I could offload to an LLM, but I rather just do it myself most of the times, and it doesn't significantly affect the time spent on the entire task---I'm a fairly fast coder when I know what I'm doing.
3. Greenfield or exploratory coding tasks the goal is not exclusively to get working code, but to learn something /through the process/ of writing it. I can obviously rely on LLMs to iterate on pieces of it, to rubber duck and evaluate alternatives, but if I just filled an initial prompt and reviewed results, I couldn't consider the work done even if the code worked, as I would be left with just superficial knowledge---weak ownership---of the system.

#+BEGIN_CENTER
\lowast{}
#+END_CENTER


<but even if llms were cheap and didn't hallucinate and could consistently produce good code, and even if we ignored that there's a lot more to development than just writing the code,
<instead: but less assume that
there would still be serious trade-offs in trying to delegate all the coding to AIs.

even if
3. There aren't any downsides to letting LLMs doing the work
   - design and economic incentive: producing more output
   - code is liability, not an asset.
   - we're better off removing code or preventing it from being written, which is the exact opposite of the llms modus operandi
   - complexity reduction
   - ironies of automation
   - knowledge building

#+BEGIN_CENTER
\lowast{}
#+END_CENTER

My understanding is that burning money to churn out broken MVPs has been a plausible business model for a long time. And I guess it may still be if you work for a startup in California. If you live in that world, then an LLM may well replace half of your programmers. Based on the headlines, it sounds as that world has ended or is coming to end. Maybe it's not, who can tell? I wouldn't make any hard bets on what software engineering will look like in a year or two.

If you are in the business of building stable software and solving problems efficiently today, you can certainly benefit from augmenting smart engineers with AI, but if you try to replace them with the AIs available today, <you'll hit a dead end, not next week or next month, but next year.

<so while the world and our industry are in heavy turmoil, I'll do what software development has thought me to do: understand what's the most effective way to solve the problems of today, with resources available today; instead of predicting what may come in the future, keep things lean and simple, and be ready to execute while the rest are recalculating.

*** Notes

[fn:6] It's not that I don't think capitalists will attempt to replace knowledge workers---or that they won't succeed at it. It's just that when they do, the proportion of bullshit jobs will increase, and the average quality of the work will get worse. That's not a future I look forward to and not something I can call progress.

[fn:7] A gptel buffer in Emacs, with Claude 3.7 sonnet. (I was previously using GPT-4o).

[fn:1] I'm less impressed by the models and their underlying technology, than by the collective knowledge-building effort that led us to produce enough data to train them.

[fn:2] TODO merge all emacs: <I recently embarked on a big Emacs config overhaul despite still being terrible at emacs lisp and with superficial knowledge of emacs internals.
< I believe it helps that I use Emacs as my "center of computing operations": since I already pay a lot of attention to my coding and writing habits, I can be deliberate and precise on how to improve them with such a malleable tool.

[fn:4] ...but that operates on stale data and is somewhat inaccurate.

[fn:5] ...that is also a lying psychopath.
