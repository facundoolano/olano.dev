---
title: Augmentation / Replacement
date: 2025-05-04
layout: post
lang: en
tags: [ai, software]
excerpt: The bicycle of the mind, with E.T. sitting in the basket.
image: bike.jpg
---
#+OPTIONS: toc:nil num:nil
#+LANGUAGE: en

In the year since [[https://jorge.olano.dev/blog/on-ai-assistance/][I first wrote]] about AI assistance, I've been using LLMs increasingly for my job, personal projects, and writing. They fill several roles for me:

- a sophisticated search engine with the most flexible of query languages and arbitrarily customizable output formats[fn:4];
- the ultimate rubber duck, one that quacks statistically generated opinions;
- a fast, knowledgeable, overeager albeit sloppy junior programmer[fn:5];
- an English proofreader with bad taste.

Beyond these specific use cases, I see much potential in the malleability of the technology and its tooling. As it continues to improve, I can see myself building more sophisticated workflows and personal applications[fn:2]; using voice instructions while pair programming, having agents run commands in the shell and integrating applications; building a local knowledge base with all my code repositories and my writings and any note I ever scribble in the margin of my editor: a second brain that I can query and look at from all angles.

If I filter all the noise---which is loud---and try to be objective, I acknowledge that even in their present flawed incarnation, LLMs may well be the most powerful human augmentation tool in the history of computing[fn:1]. But for all its current and future power, I'm still convinced that generative AI is a mediocre human replacement for intellectual and creative work[fn:6].

#+BEGIN_CENTER
\lowast{}
#+END_CENTER

I keep reading enthusiasts, both technical and non-technical, insisting that software development has already changed, that the future is now and programmers still typing code will soon be left behind.
We should instead be running agent fleets, our work now consisting of feeding them prompts, reviewing their outputs, and getting them unstuck.

But even in that imagined future where LLMs are flawless and inexpensive, I would have objections about letting the machines do all the work:

- Code is a liability, not an asset: our job is to [[a-note-on-essential-complexity][minimize complexity]], looking for ways to remove code or, better, prevent it from being written. By design and by economic incentive, LLMs push in the opposite direction.
- [[software-design-is-knowledge-building][Software design is knowledge building]]: the output of our work is organizational knowledge, the mental model the team builds, not the source code they produce. If we delegate all code writing to AIs, even if we feed them the prompts and review the results, we end up with a superficial grasp of our own systems.
- [[code-is-run-more-than-read][Code is run more than written]]: we still need humans in the loop to operate, diagnose, fix, and modify the systems. Even if we could have agents do that for us, [[https://ckrybus.com/static/papers/Bainbridge_1983_Automatica.pdf][we wouldn't be equipped]] to jump in when something fails.

The hardest part of software building is not writing code but figuring out what needs to be built: talking with colleagues, fleshing out and disambiguating product requirements, analyzing data, reading code and documentation, weighing alternatives, making a myriad of little decisions. Most of which can be enhanced by but not completely delegated to LLMs: by the time a sufficiently precise prompt can be written, most of the work would already be done. Unless your line of business is building throw-away prototypes, automating code generation is optimizing the wrong part of the process.

#+BEGIN_CENTER
\lowast{}
#+END_CENTER

As a software engineer, I see a lot of potential in embracing AI as a human augmentation tool. Not a silver bullet, not an order of magnitude increase in productivity, but more efficacy, more reach and engagement in our work. A bicycle of the mind, with E.T. sitting in the basket.

Companies that push today for AI to replace engineers will only get faster at producing bad software. They'll hit a dead end when trying to turn prototypes into products, and won't have sufficient organizational knowledge to backtrack.

So while we may be on the verge of dramatic changes in the way we go about our work, rather than trying to predict the future, let's do what software development has taught us to do: understand what is the most effective way to solve the problems of today with resources available today; keeping things lean and simple, ready to execute when most are shifting gears.

*** Notes

[fn:2] The fact that I'm an Emacs user helps here: I'm already used to paying attention to my habits, and my editor works like an almost limitless canvas to try any feature that I can imagine. As [[https://simonwillison.net/2023/Mar/27/ai-enhanced-development/][Simon Willison]] often puts it, AI can provide that extra boost of ambition to experiment with things that otherwise wouldn't be worth the time investment. This is why, as the industry seems to [[https://www.jonashietala.se/blog/2025/02/18/ill_give_up_neovim_when_you_pry_it_from_my_cold_dead_hands/][push for uniformity]], I'm encouraged to double down on personalized tools.

[fn:6] It's not that I don't think capitalists will attempt to replace knowledge workers---or that they won't succeed. It's just that when they do, the proportion of bullshit jobs will increase, and the average quality of the work will get worse. That's not a future I look forward to and not something I can call progress.

[fn:1] I'm less impressed by the models and their underlying technology than by the collective knowledge-building effort that enabled their training.

[fn:4] ...but that operates on stale data and is somewhat inaccurate.

[fn:5] ...that is also a lying psychopath.
